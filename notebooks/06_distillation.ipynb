{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from src.dataset import HumanPosesDataset\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "1120ddcca59029de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\""
   ],
   "id": "f93d2b0985b9fe1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Датасет",
   "id": "907c346b5ad4f806"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.5, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "    ], p=0.3),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.05, 0.05), scale=(0.9, 1.1))\n",
    "    ], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random'),\n",
    "    transforms.Normalize(mean=mean, std=mean)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=mean)\n",
    "])\n"
   ],
   "id": "4ab6bb4cf3bc6cef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CSV_PATH = Path(\"../data/human_poses_data/train_answers.csv\")\n",
    "TRAIN_DIR = Path(\"../data/human_poses_data/img_train\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "train_ids, val_ids = train_test_split(\n",
    "    df['img_id'].values,\n",
    "    test_size=0.2,\n",
    "    stratify=df['target_feature'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = df[df['img_id'].isin(train_ids)].reset_index(drop=True)\n",
    "val_df = df[df['img_id'].isin(val_ids)].reset_index(drop=True)\n",
    "\n",
    "train_dataset = HumanPosesDataset(\n",
    "    data_df=train_df,\n",
    "    img_dir=TRAIN_DIR,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "val_dataset = HumanPosesDataset(\n",
    "    data_df=val_df,\n",
    "    img_dir=TRAIN_DIR,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ],
   "id": "66be7fe32d4512fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_classes = len(np.unique(df['target_feature']))\n",
    "print(f\"Количество классов: {num_classes}\")"
   ],
   "id": "cfe8c940f069aba1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Переписанные функции обучения",
   "id": "9cd05968f4b9a586"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, teacher_model, temperature=4.0, alpha=0.7, ce_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher_model.eval()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.ce_smoothing = ce_smoothing\n",
    "\n",
    "    def forward(self, student_logits, _, labels):\n",
    "        if isinstance(labels, tuple) and len(labels) == 2:\n",
    "            targets, x_teacher = labels\n",
    "        else:\n",
    "            raise ValueError(\"Expected labels to be a tuple: (targets, x_teacher)\")\n",
    "\n",
    "        x_teacher = x_teacher.to(student_logits.device)\n",
    "        targets = targets.to(student_logits.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = self.teacher(x_teacher)\n",
    "\n",
    "        T = self.temperature\n",
    "        soft_teacher = F.softmax(teacher_logits / T, dim=1)\n",
    "        soft_student = F.log_softmax(student_logits / T, dim=1)\n",
    "        distill_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T ** 2)\n",
    "\n",
    "        ce_loss = F.cross_entropy(student_logits, targets, label_smoothing=self.ce_smoothing)\n",
    "        return self.alpha * distill_loss + (1 - self.alpha) * ce_loss\n",
    "\n",
    "\n",
    "def distill_batch_augment(images, labels, teacher_transform):\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    x_teacher = torch.stack([\n",
    "        teacher_transform(to_pil(img.cpu())).to(images.device)\n",
    "        for img in images\n",
    "    ])\n",
    "\n",
    "    return images, (labels, x_teacher)"
   ],
   "id": "9d4c5eaa82d34ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "\n",
    "def training_epoch(model, optimizer, criterion, train_loader, device, tqdm_desc, batch_augment_fn=None, scheduler=None, scaler=None):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=tqdm_desc):\n",
    "        images = images.to(device)\n",
    "\n",
    "        if batch_augment_fn is not None:\n",
    "            images, labels = batch_augment_fn(images, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scaler is not None:\n",
    "            with autocast(device_type='cuda'):\n",
    "                logits = model(images)\n",
    "\n",
    "                if isinstance(labels, tuple) and len(labels) == 2:\n",
    "                    targets, x_teacher = labels\n",
    "                    x_teacher = x_teacher.to(device)\n",
    "                    loss = criterion(logits, None, (targets, x_teacher))\n",
    "                    labels_for_f1 = targets\n",
    "                else:\n",
    "                    loss = criterion(logits, labels)\n",
    "                    labels_for_f1 = labels\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(images)\n",
    "\n",
    "            if isinstance(labels, tuple) and len(labels) == 2:\n",
    "                targets, x_teacher = labels\n",
    "                x_teacher = x_teacher.to(device)\n",
    "                loss = criterion(logits, None, (targets, x_teacher))\n",
    "                labels_for_f1 = targets\n",
    "            else:\n",
    "                loss = criterion(logits, labels)\n",
    "                labels_for_f1 = labels\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        all_preds.append(logits.detach().argmax(dim=1).cpu())\n",
    "        all_labels.append(labels_for_f1.cpu())\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_f1 = f1_score(torch.cat(all_labels), torch.cat(all_preds), average='macro')\n",
    "    return train_loss, train_f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_epoch(model, criterion, val_loader, device, tqdm_desc, teacher_model=None, teacher_transform=None):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    to_pil = transforms.ToPILImage()\n",
    "\n",
    "    for images, labels in tqdm(val_loader, desc=tqdm_desc):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if isinstance(criterion, DistillationLoss):\n",
    "            x_teacher = torch.stack([\n",
    "                teacher_transform(to_pil(img.cpu())).to(device)\n",
    "                for img in images\n",
    "            ])\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, None, (labels, x_teacher))\n",
    "        else:\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "        all_preds.append(logits.argmax(dim=1).cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_f1 = f1_score(torch.cat(all_labels), torch.cat(all_preds), average='macro')\n",
    "    return val_loss, val_f1"
   ],
   "id": "f905e45a8e35b31e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from src.utils import set_seed, plot_losses\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs: int,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        scheduler=None,\n",
    "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        experiment_name: str = 'experiment',\n",
    "        save_dir: str = 'checkpoints',\n",
    "        use_wandb: bool = False,\n",
    "        seed: int = 42,\n",
    "        batch_augment_fn=None,\n",
    "        scaler = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(self.device)\n",
    "        self.batch_augment_fn = batch_augment_fn\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.save_dir = save_dir\n",
    "        self.experiment_name = experiment_name\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.scaler = scaler\n",
    "\n",
    "        from torch.optim.lr_scheduler import OneCycleLR\n",
    "        self.step_scheduler_per_batch = isinstance(scheduler, OneCycleLR)\n",
    "\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_epoch = 0\n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_f1': [], 'val_f1': []\n",
    "        }\n",
    "\n",
    "        self.use_wandb = use_wandb\n",
    "        if use_wandb:\n",
    "            wandb.init(\n",
    "                project=experiment_name,\n",
    "                config={\n",
    "                    'num_epochs': num_epochs,\n",
    "                    'optimizer': str(optimizer),\n",
    "                    'device': device,\n",
    "                    'criterion': str(criterion),\n",
    "                    'scheduler': str(scheduler),\n",
    "                    'seed': seed\n",
    "                }\n",
    "            )\n",
    "\n",
    "        set_seed(seed)\n",
    "\n",
    "    def save_checkpoint(self, epoch: int, is_best: bool = False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "            'best_f1': self.best_f1,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "        if is_best:\n",
    "            path = os.path.join(self.save_dir, f'{self.experiment_name}_best.pth')\n",
    "        else:\n",
    "            path = os.path.join(self.save_dir, f'{self.experiment_name}_epoch{epoch}.pth')\n",
    "\n",
    "        torch.save(checkpoint, path)\n",
    "\n",
    "    def train(self, start_epoch: int = 1):\n",
    "        for epoch in range(start_epoch, self.num_epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{self.num_epochs}\")\n",
    "\n",
    "            train_loss, train_f1 = training_epoch(\n",
    "                self.model, self.optimizer, self.criterion,\n",
    "                self.train_loader, self.device, f\"Train {epoch}\",\n",
    "                batch_augment_fn=self.batch_augment_fn,\n",
    "                scheduler=self.scheduler if self.step_scheduler_per_batch else None,\n",
    "                scaler = self.scaler if self.scaler else None,\n",
    "            )\n",
    "\n",
    "            val_loss, val_f1 = validation_epoch(\n",
    "                model=student_model,\n",
    "                criterion=criterion,\n",
    "                val_loader=val_loader,\n",
    "                device=device,\n",
    "                tqdm_desc=f\"Val {epoch}\",\n",
    "                teacher_model=teacher_model,\n",
    "                teacher_transform=val_transform\n",
    "            )\n",
    "\n",
    "\n",
    "            if self.scheduler is not None and not self.step_scheduler_per_batch:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_f1'].append(train_f1)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "\n",
    "            metrics = {\n",
    "                'train/loss': train_loss,\n",
    "                'train/f1': train_f1,\n",
    "                'val/loss': val_loss,\n",
    "                'val/f1': val_f1,\n",
    "                'epoch': epoch\n",
    "            }\n",
    "\n",
    "            if self.use_wandb:\n",
    "                wandb.log(metrics)\n",
    "\n",
    "            if val_f1 > self.best_f1:\n",
    "                self.best_f1 = val_f1\n",
    "                self.best_epoch = epoch\n",
    "                self.save_checkpoint(epoch, is_best=True)\n",
    "\n",
    "            self.save_checkpoint(epoch)\n",
    "\n",
    "            plot_losses(\n",
    "                self.history['train_loss'],\n",
    "                self.history['val_loss'],\n",
    "                self.history['train_f1'],\n",
    "                self.history['val_f1'],\n",
    "                clear=True\n",
    "            )\n",
    "\n",
    "        print(f\"Training completed. Best Val F1: {self.best_f1:.4f} at epoch {self.best_epoch}\")\n",
    "        return self.history"
   ],
   "id": "42f8ab53e4c7eb14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Обучение",
   "id": "5e2dee4e793b4f44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")"
   ],
   "id": "bbf337878bc93b60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.models.miniconvnext import MiniConvNeXt\n",
    "from src.models.teacher import ConvNeXtTeacher\n",
    "from src.utils import load_best_model\n",
    "\n",
    "student_model = MiniConvNeXt(num_classes=16)\n",
    "student_model = student_model.to(device)\n",
    "\n",
    "teacher_model = ConvNeXtTeacher(num_classes=16)\n",
    "load_best_model(teacher_model, '../best_models/teacher.pth', device)\n",
    "teacher_model.eval().to(device)"
   ],
   "id": "4ae5d3245044bb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from torch.amp import GradScaler\n",
    "\n",
    "NUM_EPOCH = 50\n",
    "\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=NUM_EPOCH\n",
    ")\n",
    "\n",
    "criterion = DistillationLoss(\n",
    "    teacher_model=teacher_model,\n",
    "    temperature=4.0,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import MixupCutMixAugmenter\n",
    "\n",
    "mixup_cutmix_fn = MixupCutMixAugmenter(alpha=1.0, p_mixup=0.3)\n",
    "\n",
    "def mixup_then_distill(images, labels):\n",
    "    images, labels = mixup_cutmix_fn(images, labels)\n",
    "    return distill_batch_augment(images, labels, val_transform)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=student_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCH,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    batch_augment_fn=lambda x, y: distill_batch_augment(x, y, val_transform),\n",
    "    experiment_name=\"distillation_1\",\n",
    "    use_wandb=True,\n",
    "    seed=42,\n",
    "    scaler=scaler,\n",
    ")\n",
    "\n",
    "history = trainer.train()"
   ],
   "id": "c7d924538f5b748f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import load_best_model\n",
    "\n",
    "load_best_model(student_model, 'checkpoints/distillation_1_best.pth', device)"
   ],
   "id": "cab14f72bd5f3369",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.amp import GradScaler\n",
    "\n",
    "NUM_EPOCH = 25\n",
    "\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=NUM_EPOCH\n",
    ")\n",
    "\n",
    "criterion = DistillationLoss(\n",
    "    teacher_model=teacher_model,\n",
    "    temperature=4.0,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n"
   ],
   "id": "2df0a6a7021c35e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import MixupCutMixAugmenter\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=student_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCH,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    batch_augment_fn=lambda x, y: distill_batch_augment(x, y, val_transform),\n",
    "    experiment_name=\"distillation_1_next\",\n",
    "    use_wandb=True,\n",
    "    seed=42,\n",
    "    scaler=scaler,\n",
    ")\n",
    "\n",
    "history = trainer.train()"
   ],
   "id": "4becd5d124828aff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fcd5f084806362bc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
