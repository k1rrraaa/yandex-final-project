{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, zis, zjs):\n",
    "        batch_size = zis.size(0)\n",
    "        device = zis.device\n",
    "\n",
    "        zis = F.normalize(zis, dim=1)\n",
    "        zjs = F.normalize(zjs, dim=1)\n",
    "\n",
    "        z = torch.cat([zis, zjs], dim=0)\n",
    "\n",
    "        similarity_matrix = torch.matmul(z, z.T)\n",
    "\n",
    "        mask = torch.eye(2 * batch_size, device=device).bool()\n",
    "        similarity_matrix = similarity_matrix.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        logits = similarity_matrix / self.temperature\n",
    "\n",
    "        labels = torch.arange(batch_size, device=device)\n",
    "        labels = torch.cat([labels + batch_size, labels], dim=0)\n",
    "\n",
    "        # Loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n"
   ],
   "id": "2d694bfbc5f01271",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_simclr(model, dataloader, loss_fn, optimizer, scheduler, epochs=10):\n",
    "    model.train()\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for x1, x2 in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                z1 = model(x1)\n",
    "                z2 = model(x2)\n",
    "                loss = loss_fn(z1, z2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:\n",
    "            torch.save(model.encoder.state_dict(), f\"encoder_ssl_epoch{epoch+1}.pt\")\n",
    "            print(f\"✅ Saved checkpoint: encoder_ssl_epoch{epoch+1}.pt\")\n"
   ],
   "id": "82c69ac289d22da2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "812153118aaf9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")"
   ],
   "id": "620f7081e8aea892",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(channels, eps=eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.norm(x)\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class ConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "        self.norm = LayerNorm2d(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Conv2d(dim, 4 * dim, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Conv2d(4 * dim, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class MiniConvNeXt(nn.Module):\n",
    "    def __init__(self, in_chans=3, num_classes=16,\n",
    "                 depths=None, dims=None):\n",
    "        super().__init__()\n",
    "        if depths is None:\n",
    "            depths = [2, 2, 2]\n",
    "        if dims is None:\n",
    "            dims = [64, 128, 256]\n",
    "        self.num_classes = num_classes\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm2d(dims[0], eps=1e-6)\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "        for i in range(2):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                LayerNorm2d(dims[i], eps=1e-6),\n",
    "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList([\n",
    "            nn.Sequential(*[ConvNeXtBlock(dim) for _ in range(depth)])\n",
    "            for dim, depth in zip(dims, depths)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)\n",
    "        self.head = nn.Linear(dims[-1], num_classes) if num_classes is not None else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for down, stage in zip(self.downsample_layers, self.stages):\n",
    "            x = down(x)\n",
    "            x = stage(x)\n",
    "        x = x.mean([-2, -1])\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class SimCLRModel(nn.Module):\n",
    "    def __init__(self, encoder: MiniConvNeXt, projection_dim=128):\n",
    "        super().__init__()\n",
    "        encoder_output_dim = encoder.norm.normalized_shape[0]\n",
    "        self.encoder = encoder\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(encoder_output_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder.forward_features(x)\n",
    "        projections = self.projection_head(features)\n",
    "        return projections"
   ],
   "id": "6dab21e8a2d66d87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.ssl_dataset import SimCLRDataset\n",
    "\n",
    "image_dir = '../data/human_poses_data/img_train'\n",
    "dataset = SimCLRDataset(image_folder='../data/human_poses_data/img_train')\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "encoder = MiniConvNeXt(num_classes=None).to(device)\n",
    "model = SimCLRModel(encoder).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=NUM_EPOCHS, eta_min=1e-5\n",
    ")\n",
    "\n",
    "loss_fn = NTXentLoss()\n",
    "\n",
    "\n",
    "train_simclr(model, dataloader, loss_fn, optimizer, scheduler, epochs=NUM_EPOCHS)"
   ],
   "id": "858f378305a0ab13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # # Full fine-tune",
   "id": "c2c48520e3aa1b9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.models.miniconvnext import MiniConvNeXt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from src.trainer import Trainer\n",
    "from src.dataset import HumanPosesDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ],
   "id": "d73dd4280f013bdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\""
   ],
   "id": "72662d508c5143db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Датасет",
   "id": "622e2c1daef427db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomApply([transforms.RandomErasing()], p=0.3),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "val_transform= transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])"
   ],
   "id": "defe9d5ad8e575e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CSV_PATH = Path(\"../data/human_poses_data/train_answers.csv\")\n",
    "TRAIN_DIR = Path(\"../data/human_poses_data/img_train\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "train_ids, val_ids = train_test_split(\n",
    "    df['img_id'].values,\n",
    "    test_size=0.2,\n",
    "    stratify=df['target_feature'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = df[df['img_id'].isin(train_ids)].reset_index(drop=True)\n",
    "val_df = df[df['img_id'].isin(val_ids)].reset_index(drop=True)\n",
    "\n",
    "train_dataset = HumanPosesDataset(\n",
    "    data_df=train_df,\n",
    "    img_dir=TRAIN_DIR,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "val_dataset = HumanPosesDataset(\n",
    "    data_df=val_df,\n",
    "    img_dir=TRAIN_DIR,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ],
   "id": "45db81d73825baea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_classes = len(np.unique(df['target_feature']))\n",
    "print(f\"Количество классов: {num_classes}\")"
   ],
   "id": "d4ed3db48b9f6504",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Модель",
   "id": "b640a728823e5d08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")"
   ],
   "id": "2888e3de3c4232f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "encoder = MiniConvNeXt(num_classes=None)\n",
    "encoder.load_state_dict(torch.load(\"encoder_ssl_epoch25.pt\"), strict=False)\n",
    "\n",
    "model = MiniConvNeXt(num_classes=16)\n",
    "model.load_state_dict(torch.load(\"encoder_ssl_epoch25.pt\"), strict=False)\n",
    "\n",
    "model.to(device)"
   ],
   "id": "43d72d296b958d6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.amp import GradScaler\n",
    "\n",
    "NUM_EPOCH = 75\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-4,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=NUM_EPOCH,\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1e4\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "scaler = GradScaler()"
   ],
   "id": "b73e0d5ea6bc476d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import MixupCutMixAugmenter\n",
    "\n",
    "mixup_cutmix_fn = MixupCutMixAugmenter(alpha=1.0, p_mixup=0.3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCH,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    batch_augment_fn=mixup_cutmix_fn,\n",
    "    experiment_name=\"ssl_1_1\",\n",
    "    use_wandb=True,\n",
    "    seed=42,\n",
    "    scaler=scaler\n",
    ")\n",
    "\n",
    "history = trainer.train()"
   ],
   "id": "70fa5d0c8aff2520",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import load_best_model\n",
    "\n",
    "load_best_model(model, 'checkpoints/ssl_1_1_best.pth')"
   ],
   "id": "f719b843a5a88556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.amp import GradScaler\n",
    "\n",
    "NUM_EPOCH = 75\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-4,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=NUM_EPOCH,\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1e4\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "scaler = GradScaler()"
   ],
   "id": "1752bbb8c58333b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import MixupCutMixAugmenter\n",
    "\n",
    "mixup_cutmix_fn = MixupCutMixAugmenter(alpha=1.0, p_mixup=0.3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCH,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    batch_augment_fn=mixup_cutmix_fn,\n",
    "    experiment_name=\"ssl_1_2\",\n",
    "    use_wandb=True,\n",
    "    seed=42,\n",
    "    scaler=scaler\n",
    ")\n",
    "\n",
    "history = trainer.train()"
   ],
   "id": "fad6760a80a3c096",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f3214792f3a11e32",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
